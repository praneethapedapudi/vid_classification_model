# -*- coding: utf-8 -*-
"""vid_classification_using_timesformerk600.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gEnyvS0c9sWSjem1LkK6gQs5_rJygWa0
"""

!pip install transformers
!pip install torch torchvision
!pip install opencv-python

!pip install transformers torch==2.3.0 torchvision==0.14.1 torchaudio==0.13.1 opencv-python-headless numpy

from transformers import AutoImageProcessor, TimesformerForVideoClassification
import numpy as np
import torch
import cv2
from google.colab import files

processor = AutoImageProcessor.from_pretrained("facebook/timesformer-base-finetuned-k600")
model = TimesformerForVideoClassification.from_pretrained("facebook/timesformer-base-finetuned-k600")

def extract_frames(video_path, num_frames_needed=8):
    video_capture = cv2.VideoCapture(video_path)
    frames = []
    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))
    interval = max(1, total_frames // num_frames_needed)

    for i in range(num_frames_needed):
        video_capture.set(cv2.CAP_PROP_POS_FRAMES, i * interval)
        success, frame = video_capture.read()
        if success:
            frame = cv2.resize(frame, (224, 224))
            frame = frame[:, :, ::-1]  # BGR to RGB
            frames.append(frame)
        else:
            break

    video_capture.release()
    return frames

uploaded = files.upload()
video_path = next(iter(uploaded))

frames = extract_frames(video_path)

num_frames = len(frames)
if num_frames < 8:
    raise ValueError("The video must contain at least 8 frames.")
else:
    # Select exactly 8 frames
    video = [np.transpose(frame, (2, 0, 1)) for frame in frames[:8]]  # Convert to (C, H, W)

inputs = processor(images=video, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits

predicted_class_idx = logits.argmax(-1).item()
print("Predicted class:", model.config.id2label[predicted_class_idx])

